---
title: 'Midterm Exam'
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r libraries, message=FALSE}
# SEE modeldata package for new datasets
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date manipulation
library(dbplyr)            # for SQL query "cheating" - part of tidyverse but needs to be loaded separately
library(mdsr)              # for accessing some databases - goes with Modern Data Science with R textbook

#tidytext
library(tidytext)          # for text analysis, the tidy way!
library(textdata)          
library(reshape2)
library(wordcloud)         # for wordcloud
library(stopwords)

theme_set(theme_minimal()) # Lisa's favorite theme
```

When you finish the exam, remove the `#` from the options chunk at the top, so that messages and warnings aren't printed. If you are getting errors in your code, add `error = TRUE` so that the file knits. I would recommend not removing the `#` until you are completely finished.


## Text Analysis with Bias


**`tidytext` tasks**:

You will try using tidytext on a new dataset about Russian Troll tweets.

#### Read about the data

These are tweets from Twitter handles that are connected to the Internet Research Agency (IRA), a Russian "troll factory."  The majority of these tweets were posted from 2015-2017, but the datasets encompass tweets from February 2012 to May 2018.

Three of the main categories of troll tweet that we will be focusing on are Left Trolls, Right Trolls, and News Feed.  **Left Trolls** usually pretend to be BLM activists, aiming to divide the democratic party (in this context, being pro-Bernie so that votes are taken away from Hillary).  **Right trolls** imitate Trump supporters, and **News Feed** handles are "local news aggregators," typically linking to legitimate news.

For our upcoming analyses, some important variables are:

  * **author** (handle sending the tweet)
  * **content** (text of the tweet)
  * **language** (language of the tweet)
  * **publish_date** (date and time the tweet was sent)

Variable documentation can be found on [Github](https://github.com/fivethirtyeight/russian-troll-tweets/) and a more detailed description of the dataset can be found in this [fivethirtyeight article](https://fivethirtyeight.com/features/why-were-sharing-3-million-russian-troll-tweets/).

Because there are 12 datasets containing 2,973,371 tweets sent by 2,848 Twitter handles in total, we will be using three of these datasets (one from a Right troll, one from a Left troll, and one from a News Feed account).

\
\

1. Read in Troll Tweets Dataset - this takes a while. You can cache it so you don't need to read it in again each time you knit. Be sure to remove the `eval=FALSE`!!!!

```{r}
troll_tweets <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv")
```

2. Basic Data Cleaning and Exploration

  a. Remove rows where the tweet was in a language other than English
  
```{r}
troll_tweets <- troll_tweets[troll_tweets$language == "English", ]
```

  b. Report the dimensions of the dataset

```{r}
dim(troll_tweets)
```

  c. Create two or three basic exploratory plots of the data (ex. plot of the different locations from which tweets were posted, plot of the account category of a tweet)
  
```{r}
region_count <- troll_tweets %>% group_by(region) %>% summarise(total_count = n (), .groups = "keep") %>% as.data.frame()
```

```{r}
region_plot <- ggplot(region_count, aes(x = region, y = total_count)) +
  geom_point() +
  labs(title = "Total Tweets per Region",
       subtitle = "Russion Troll Tweets",
       caption = "Data Source:") +
  labs(y = "Number of Tweets", x = "Region")
region_plot + theme(axis.text.x = element_text(angle = 90))
                                               
```

3. Unnest Tokens

We want each row to represent a word from a tweet, rather than an entire tweet. Be sure to remove the `eval=FALSE`!!!!

```{r, eval = FALSE}
troll_tweets_untoken <- troll_tweets1 %>%
  unnest_tokens(???,???)

troll_tweets_untoken
```

\
\

4. Remove stopwords. Be sure to remove the `eval=FALSE`!!!!

```{r, eval = FALSE}
#get rid of stopwords (the, and, etc.)
troll_tweets_cleaned <- troll_tweets_untoken %>%
  anti_join(stop_words)
```

Take a look at the troll_tweets_cleaned dataset.  Are there any other words/letters/numbers that we want to eliminate that weren't taken care of by stop_words? Be sure to remove the `eval=FALSE`!!!!

```{r, eval = FALSE}
#get rid of http, https, t.co, rt, amp, single number digits, and singular letters
troll_tweets_cleaned <- troll_tweets_cleaned %>%
  filter(word != ????) # you can use %in% for a list of words
```


5. Look at a subset of the tweets to see how often the top words appear.

```{r, eval = FALSE}
troll_tweets_small <- troll_tweets_cleaned %>%
  count(??) %>%
  slice_max(order_by = n, n = 50) # 50 most occurring words

# visualize the number of times the 50 top words appear
ggplot(troll_tweets_small, 
       aes(y = fct_reorder(word,n), x = n)) +
  geom_col()
```


6. Sentiment Analysis

  a. Get the sentiments using the "bing" parameter (which classifies words into "positive" or "negative")
  b. Report how many positive and negative words there are in the dataset.  Are there more positive or negative words, and why do you think this might be?
  
Be sure to remove the `eval=FALSE`!!!!

```{r, eval = FALSE}
# look at sentiment
get_sentiments("bing")

# assign a sentiment to each word that has one associated
troll_tweets_sentiment <- troll_tweets_cleaned %>%
  inner_join(???)

# count the sentiments
troll_tweets_sentiment %>% 
  ???
```

7. Using the troll_tweets_small dataset, make a wordcloud:

  a. That is sized by the number of times that a word appears in the tweets
  b. That is colored by sentiment (positive or negative)


Be sure to remove the `eval=FALSE`!!!!

```{r, eval = FALSE}
# make a wordcloud where the size of the word is based on the number of times the word appears across the tweets

troll_tweets_small %>%
  with(wordcloud(word, n, max.words = ??))

# make a wordcloud colored by sentiment

troll_tweets_sentiment %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c(??,??),
                   max.words = ??)
```

Are there any words whose categorization as "positive" or "negative" surprised you?


## "Undoing" bias

**Task:**

Read this tweet [thread](https://threadreaderapp.com/thread/1375957284061376516.html) by [Deb Raji](https://en.wikipedia.org/wiki/Deborah_Raji) who you may remember from the *Coded Bias* film. Write a short paragraph that discusses at least one of the misconceptions.